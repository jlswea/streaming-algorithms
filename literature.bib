@inproceedings{Babcock2002,
  author    = {Brian Babcock and Shivnath Babu and Mayur Datar and Rajeev Motwani and Jennifer Widom},
  title     = {Models and issues in data stream systems},
  booktitle = {IN PODS},
  year      = {2002},
  pages     = {1--16},
  publisher = {}
}

@online{ITS1996, 
  author    = {Institute for Telecommunication Sciences},
  title     = {FS-1037C},
  year      = {1996},
  url       = {https://www.its.bldrdoc.gov/fs-1037/dir-010/_1451.htm},
  url-date  = {2020-12-19}
}

@article{Karp2003, 
  author = {Karp, Richard M. and Shenker, Scott and Papadimitriou, Christos H.}, 
  title = {A Simple Algorithm for Finding Frequent Elements in Streams and Bags}, 
  year = {2003}, issue_date = {March 2003}, 
  publisher = {Association for Computing Machinery}, 
  address = {New York, NY, USA}, 
  volume = {28}, 
  number = {1}, 
  issn = {0362-5915}, 
  url = {https://doi.org/10.1145/762471.762473}, 
  doi = {10.1145/762471.762473}, 
  abstract = {We present a simple, exact algorithm for identifying in a multiset the items with frequency more than a threshold θ. The algorithm requires two passes, linear time, and space 1/θ. The first pass is an on-line algorithm, generalizing a well-known algorithm for finding a majority element, for identifying a set of at most 1/θ items that includes, possibly among others, all items with frequency greater than θ.}, 
  journal = {ACM Trans. Database Syst.}, 
  month = mar, 
  pages = {51–55}, 
  numpages = {5}, 
  keywords = {frequent elements, Data stream} 
}

@article{Woodruff2014,
  author    = {David P. Woodruff},
  title     = {Data Streams and Applications in Computer Science},
  journal   = {Bull. {EATCS}},
  volume    = {114},
  year      = {2014},
  url       = {http://eatcs.org/beatcs/index.php/beatcs/article/view/304},
  timestamp = {Thu, 18 Jun 2020 22:07:15 +0200},
  biburl    = {https://dblp.org/rec/journals/eatcs/Woodruff14.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{Muthu2005, 
  author = {Muthukrishnan, S.}, 
  title = {Data Streams: Algorithms and Applications}, 
  year = {2005}, 
  issue_date = {August 2005}, 
  publisher = {Now Publishers Inc.}, 
  address = {Hanover, MA, USA}, 
  volume = {1}, 
  number = {2}, 
  issn = {1551-305X}, 
  url = {https://doi.org/10.1561/0400000002}, 
  doi = {10.1561/0400000002}, 
  abstract = {In the data stream scenario, input arrives very rapidly and there is limited memory to store the input. Algorithms have to work with one or few passes over the data, space less than linear in the input size or time significantly less than the input size. In the past few years, a new theory has emerged for reasoning about algorithms that work within these constraints on space, time, and number of passes. Some of the methods rely on metric embeddings, pseudo-random computations, sparse approximation theory and communication complexity. The applications for this scenario include IP network traffic analysis, mining text message streams and processing massive data sets in general. Researchers in Theoretical Computer Science, Databases, IP Networking and Computer Systems are working on the data stream challenges. This article is an overview and survey of data stream algorithmics and is an updated version of [1].}, 
  journal = {Found. Trends Theor. Comput. Sci.}, 
  month = aug, 
  pages = {117–236}, 
  numpages = {120} 
}

@online{Chakra2020,
  author = {Chakrabarti, Amit},
  title = {Data Stream Algorithms, Lecture notes},
  year = {2020},
  publisher = {Dartmouth College},
  url = {https://www.cs.dartmouth.edu/~ac/Teach/CS35-Spring20/},
  url-date  = {2020-12-21}
}

@article{Alon1999,
  title = "The Space Complexity of Approximating the Frequency Moments",
  journal = "Journal of Computer and System Sciences",
  volume = "58",
  number = "1",
  pages = "137 - 147",
  year = "1999",
  issn = "0022-0000",
  doi = "https://doi.org/10.1006/jcss.1997.1545",
  url = "http://www.sciencedirect.com/science/article/pii/S0022000097915452",
  author = "Noga Alon and Yossi Matias and Mario Szegedy",
  abstract = "The frequency moments of a sequence containingmielements of typei, 1⩽i⩽n, are the numbersFk=∑ni=1mki. We consider the space complexity of randomized algorithms that approximate the numbersFk, when the elements of the sequence are given one by one and cannot be stored. Surprisingly, it turns out that the numbersF0,F1, andF2can be approximated in logarithmic space, whereas the approximation ofFkfork⩾6 requiresnΩ(1)space. Applications to data bases are mentioned as well."
}

@article{Blum1973, 
  author = {Blum, Manuel and Floyd, Robert W. and Pratt, Vaughan and Rivest, Ronald L. and Tarjan, Robert E.}, 
  title = {Time Bounds for Selection}, 
  year = {1973}, 
  issue_date = {August, 1973}, 
  publisher = {Academic Press, Inc.}, 
  address = {USA}, 
  volume = {7}, 
  number = {4}, 
  issn = {0022-0000}, 
  url = {https://doi.org/10.1016/S0022-0000(73)80033-9}, 
  doi = {10.1016/S0022-0000(73)80033-9}, 
  abstract = {The number of comparisons required to select the i-th smallest of n numbers is shown to be at most a linear function of n by analysis of a new selection algorithm-PICK. Specifically, no more than 5.4305 n comparisons are ever required. This bound is improved for extreme values of i, and a new lower bound on the requisite number of comparisons is also proved.}, 
  journal = {J. Comput. Syst. Sci.}, 
  month = aug, 
  pages = {448–461}, 
  numpages = {14} 
}

@article{Munro1980,
  title = "Selection and sorting with limited storage",
  journal = "Theoretical Computer Science",
  volume = "12",
  number = "3",
  pages = "315 - 323",
  year = "1980",
  issn = "0304-3975",
  doi = "https://doi.org/10.1016/0304-3975(80)90061-4",
  url = "http://www.sciencedirect.com/science/article/pii/0304397580900614",
  author = "J.I. Munro and M.S. Paterson",
  abstract = "When selecting from, or sorting, a file stored on a read-only tape and the internal storage is rather limited, several passes of the input tape may be required. We study the relation between the amount of internal storage available and the number of passes required to select the Kth highest of N inputs. We show, for example, that to find the median in two passes requires at least ω(N12) and at most O(N12log N) internal storage. For probabilistic methods, θ(N12) internal storage is necessary and sufficient for a single pass method which finds the median with arbitrarily high probability."
}

@inproceedings{Gilbert2001, 
  author = {Gilbert, Anna C. and Kotidis, Yannis and Muthukrishnan, S. and Strauss, Martin}, 
  title = {Surfing Wavelets on Streams: One-Pass Summaries for Approximate Aggregate Queries}, 
  year = {2001}, 
  isbn = {1558608044}, 
  publisher = {Morgan Kaufmann Publishers Inc.}, 
  address = {San Francisco, CA, USA}, 
  booktitle = {Proceedings of the 27th International Conference on Very Large Data Bases}, 
  pages = {79–88}, 
  numpages = {10}, 
  series = {VLDB '01} 
}

@article{Chaud1998, 
  author = {Chaudhuri, Surajit and Motwani, Rajeev and Narasayya, Vivek}, 
  title = {Random Sampling for Histogram Construction: How Much is Enough?}, 
  year = {1998}, 
  issue_date = {June 1998}, 
  publisher = {Association for Computing Machinery}, 
  address = {New York, NY, USA}, 
  volume = {27}, 
  number = {2}, 
  issn = {0163-5808}, 
  url = {https://doi.org/10.1145/276305.276343}, 
  doi = {10.1145/276305.276343}, 
  abstract = {Random sampling is a standard technique for constructing (approximate) histograms for query optimization. However, any real implementation in commercial products requires solving the hard problem of determining “How much sampling is enough?” We address this critical question in the context of equi-height histograms used in many commercial products, including Microsoft SQL Server. We introduce a conservative error metric capturing the intuition that for an approximate histogram to have low error, the error must be small in all regions of the histogram. We then present a result establishing an optimal bound on the amount of sampling required for pre-specified error bounds. We also describe an adaptive page sampling algorithm which achieves greater efficiency by using all values in a sampled page but adjusts the amount of sampling depending on clustering of values in pages. Next, we establish that the problem of estimating the number of distinct values is provably difficult, but propose a new error metric which has a reliable estimator and can still be exploited by query optimizers to influence the choice of execution plans. The algorithm for histogram construction was prototyped on Microsoft SQL Server 7.0 and we present experimental results showing that the adaptive algorithm accurately approximates the true histogram over different data distributions.}, 
  journal = {SIGMOD Rec.}, 
  month = jun, 
  pages = {436–447}, 
  numpages = {12} 
}

@inproceedings{Cormode2017,
  title={What is Data Sketching , and Why Should I Care ?},
  author={Graham Cormode},
  year={2017}
}

@article{Cormode2009, 
  author = {Cormode, Graham and Hadjieleftheriou, Marios}, 
  title = {Finding the Frequent Items in Streams of Data}, 
  year = {2009}, 
  issue_date = {October 2009}, 
  publisher = {Association for Computing Machinery}, 
  address = {New York, NY, USA}, 
  volume = {52}, 
  number = {10}, 
  issn = {0001-0782}, 
  url = {https://doi.org/10.1145/1562764.1562789}, 
  doi = {10.1145/1562764.1562789}, 
  abstract = {Many data generation processes can be modeled as data streams. They produce huge numbers of pieces of data, each of which is simple in isolation, but which taken together lead to a complex whole. For example, the sequence of queries posed to an Internet search engine can be thought of as a stream, as can the collection of transactions across all branches of a supermarket chain. In aggregate, this data can arrive at enormous rates, easily in the realm of hundreds of gigabytes per day or higher. While this data may be archived and indexed within a data warehouse, it is also important to process the data "as it happens," to provide up to the minute analysis and statistics on current trends. Methods to achieve this must be quick to respond to each new piece of information, and use resources which are very small when compared to the total quantity of data.These applications and others like them have led to the formulation of the so-called "streaming model." In this abstraction, algorithms take only a single pass over their input, and must accurately compute various functions while using resources (space and time per item) that are strictly sublinear in the size of the input---ideally, polynomial in the logarithm of the input size. The output must be produced at the end of the stream, or when queried on the prefix of the stream that has been observed so far. (Other variations ask for the output to be maintained continuously in the presence of updates, or on a "sliding window" of only the most recent updates.) Some problems are simple in this model: for example, given a stream of transactions, finding the mean and standard deviation of the bill totals can be accomplished by retaining a few "sufficient statistics" (sum of all values, sum of squared values, etc.). Others can be shown to require a large amount of information to be stored, such as determining whether a particular search query has already appeared anywhere within a large stream of queries. Determining which problems can be solved effectively within this model remains an active research area.The frequent items problem (also known as the heavy hitters problem) is one of the most heavily studied questions in data streams. The problem is popular due to its simplicity to state, and its intuitive interest and value. It is important both in itself, and as a subroutine within more advanced data stream computations. Informally, given a sequence of items, the problem is simply to find those items which occur most frequently. Typically, this is formalized as finding all items whose frequency exceeds a specified fraction of the total number of items. This is shown in Figure 1. Variations arise when the items are given weights, and further when these weights can also be negative.This abstract problem captures a wide variety of settings. The items can represent packets on the Internet, and the weights are the size of the packets. Then the frequent items represent the most popular destinations, or the heaviest bandwidth users (depending on how the items are extracted from the flow identifiers). This knowledge can help in optimizing routing decisions, for in-network caching, and for planning where to add new capacity. Or, the items can represent queries made to an Internet search engine, and the frequent items are now the (currently) popular terms. These are not simply hypothetical examples, but genuine cases where algorithms for this problem have been applied by large corporations: AT&T and Google, respectively. Given the size of the data (which is being generated at high speed), it is important to find algorithms which are capable of processing each new update very quickly, without blocking. It also helps if the working space of the algorithm is very small, so that the analysis can happen over many different groups in parallel, and because small structures are likely to have better cache behavior and hence further help increase the throughput.Obtaining efficient and scalable solutions to the frequent items problem is also important since many streaming applications need to find frequent items as a subroutine of another, more complex computation. Most directly, mining frequent itemsets inherently builds on finding frequent items as a basic building block. Finding the entropy of a stream requires learning the most frequent items in order to directly compute their contribution to the entropy, and remove their contribution before approximating the entropy of the residual stream. The HSS (Hierarchical Sampling from Sketches) technique uses hashing to derive multiple substreams, the frequent elements of which are extracted to estimate the frequency moments of the stream. The frequent items problem is also related to the recently popular area of Compressed Sensing.Other work solves generalized versions of frequent items problems by building on algorithms for the "vanilla" version of the problem. Several techniques for finding the frequent items in a "sliding window" of recent updates (instead of all updates) operate by keeping track of the frequent items in many sub-windows. In the "heavy hitters distinct" problem, with applications to detecting network scanning attacks, the count of an item is the number of distinct pairs containing that item paired with a secondary item. It is typically solved extending a frequent items algorithm with distinct counting algorithms. Frequent items have also been applied to models of probabilistic streaming data, and within faster "skipping" techniques.Thus the problem is an important one to understand and study in order to produce efficient streaming implementations. It remains an active area, with many new research contributions produced every year on the core problem and its variations. Due to the amount of work on this problem, it is easy to miss out some important references or fail to appreciate the properties of certain algorithms. There are several cases where algorithms first published in the 1980s have been "rediscovered" two decades later; existing work is sometimes claimed to be incapable of a certain guarantee, which in truth it can provide with only minor modifications; and experimental evaluations do not always compare against the most suitable methods.In this paper, we present the main ideas in this area, by describing some of the most significant algorithms for the core problem of finding frequent items using common notation and terminology. In doing so, we also present the historical development of these algorithms. Studying these algorithms is instructive, as they are relatively simple, but can be shown to provide formal guarantees on the quality of their output as a function of an accuracy parameter ε. We also provide baseline implementations of many of these algorithms against which future algorithms can be compared, and on top of which algorithms for different problems can be built. We perform experimental evaluation of the algorithms over a variety of data sets to indicate their performance in practice. From this, we are able to identify clear distinctions among the algorithms that are not apparent from their theoretical analysis alone.}, 
  journal = {Commun. ACM}, 
  month = oct, 
  pages = {97–105}, 
  numpages = {9} 
}

@online{Tipp2012,
  author = {Tippenhauer, Simon},
  title = {Seminar über Algorithmen - Streaming Algorithmen 2},
  year = {2012},
  publisher = {Freie Universität Berlin},
  url = {http://www.inf.fu-berlin.de/lehre/SS12/SemAlg/Seminar_SS12_Präsentation_Tippenhauer.pdf},
  url-date  = {2021-01-14}
}

@article{Misra_1982,
title = "Finding repeated elements",
journal = "Science of Computer Programming",
volume = "2",
number = "2",
pages = "143 - 152",
year = "1982",
issn = "0167-6423",
doi = "https://doi.org/10.1016/0167-6423(82)90012-0",
url = "http://www.sciencedirect.com/science/article/pii/0167642382900120",
author = "J. Misra and David Gries",
abstract = "Two algorithms are presented for binding the values that occur more than n ÷ k times in an array b[0:n – 1]. The second one requires time proportional to n ∗ log(k) and extra space proportional to k. A theorem suggests that this algorithm is optimal among algorithms that are based on comparing array elements. Thus, finding the element that occurs more than n ÷ 2 times requires linear time, while determining whether there is a duplicate – the case k = n – requires time proportional to n ∗ log n. The algorithms may be interesting from a standpoint of programming methodology; each was developed as an extension of the algorithm for the simple case k = 2."
}

@Inbook{Boyer_1991,
author="Boyer, Robert S.
and Moore, J. Strother",
editor="Boyer, Robert S.",
title="MJRTY---A Fast Majority Vote Algorithm",
bookTitle="Automated Reasoning: Essays in Honor of Woody Bledsoe",
year="1991",
publisher="Springer Netherlands",
address="Dordrecht",
pages="105--117",
abstract="A new algorithm is presented for determining which, if any, of an arbitrary number of candidates has received a majority of the votes cast in an election. The number of comparisons required is at most twice the number of votes. Furthermore, the algorithm uses storage in a way that permits an efficient use of magnetic tape. A Fortran version of the algorithm is exhibited. The Fortran code has been proved correct by a mechanical verification system for Fortran. The system and the proof are discussed.",
isbn="978-94-011-3488-0",
doi="10.1007/978-94-011-3488-0_5",
url="https://doi.org/10.1007/978-94-011-3488-0_5"
}

@misc{Nieße2020,
  author = {Nieße, Astrid},
  title = {Datenstrukturen und Algorithmen},
  year = {2020},
  howpublished = {University Lecture},
  publisher = {Leibniz Universität Hannover},
  url = {http://www.ei.uni-hannover.de/teaching/},
  url-date  = {2021-01-25}
}

@book{Knuth1998, 
  author = {Knuth, Donald E.}, 
  title = {The Art of Computer Programming, Volume 3: (2nd Ed.) Sorting and Searching}, 
  year = {1998}, 
  isbn = {0201896850}, 
  publisher = {Addison Wesley Longman Publishing Co., Inc.}, 
  address = {USA} 
}

@misc{Meier2020,
  author = {Meier, Arne},
  title = {Effiziente Algorithmen},
  year = {2020},
  howpublished = {University Lecture},
  publisher = {Leibniz Universität Hannover},
  url = {https://www.thi.uni-hannover.de/de/lehre/msc/algo/},
  url-date  = {2021-02-11}
}

@article{Good1989,
  author = {Good, I.J.},
  title = {C332. Surprise indexes and p-values},
  journal = {Journal of Statistical Computation and Simulation},
  volume = {32},
  number = {1-2},
  pages = {90-92},
  year  = {1989},
  publisher = {Taylor & Francis},
  doi = {10.1080/00949658908811160},
  URL = {https://doi.org/10.1080/00949658908811160},
  eprint = {https://doi.org/10.1080/00949658908811160}
}

@article{Flajolet1985,
  title = {Probabilistic counting algorithms for data base applications},
  journal = {Journal of Computer and System Sciences},
  volume = {31},
  number = {2},
  pages = {182-209},
  year = {1985},
  issn = {0022-0000},
  doi = {https://doi.org/10.1016/0022-0000(85)90041-8},
  url = {https://www.sciencedirect.com/science/article/pii/0022000085900418},
  author = {Philippe Flajolet and G. {Nigel Martin}},
  abstract = {This paper introduces a class of probabilistic counting algorithms with which one can estimate the number of distinct elements in a large collection of data (typically a large file stored on disk) in a single pass using only a small additional storage (typically less than a hundred binary words) and only a few operations per element scanned. The algorithms are based on statistical observations made on bits of hashed values of records. They are by construction totally insensitive to the replicative structure of elements in the file; they can be used in the context of distributed systems without any degradation of performances and prove especially useful in the context of data bases query optimisation.}
}

@inproceedings{Chariker2002,
    author = {Charikar, Moses and Chen, Kevin and Farach-Colton, Martin},
    title = {Finding Frequent Items in Data Streams},
    year = {2002},
    isbn = {3540438645},
    publisher = {Springer-Verlag},
    address = {Berlin, Heidelberg},
    abstract = {We present a 1-pass algorithm for estimating the most frequent items in a data stream using very limited storage space. Our method relies on a novel data structure called a count sketch, which allows us to estimate the frequencies of all the items in the stream. Our algorithm achieves better space bounds than the previous best known algorithms for this problem for many natural distributions on the item frequencies. In addition, our algorithm leads directly to a 2-pass algorithm for the problem of estimating the items with the largest (absolute) change in frequency between two data streams. To our knowledge, this problem has not been previously studied in the literature.},
    booktitle = {Proceedings of the 29th International Colloquium on Automata, Languages and Programming},
    pages = {693–703},
    numpages = {11},
    series = {ICALP '02}
}